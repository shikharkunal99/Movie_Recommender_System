data is split in the ratio 80% traning set 20% validation set

original model -
x = merge([u, m], mode='concat')
x = Flatten()(x)
x = Dropout(0.3)(x)
x = Dense(70, activation='relu')(x)
x = Dropout(0.75)(x)
x = Dense(1)(x)
nn = Model([user_in, movie_in], x)
nn.compile(Adam(0.001), loss='mse')

model 1 -
x = merge([u, m], mode='concat')
x = Flatten()(x)
x = Dropout(0.35)(x)
x = Dense(500,activation='relu')(x)
x = Dropout(0.5)(x)
x = Dense(100,activation='relu')(x)
x = Dropout(0.5)(x)
x = Dense(10,activation='relu')(x)
x = Dense(1,activation='relu')(x)
nn = Model([user_in, movie_in], x)
nn.compile(Adam(0.001), loss='mse')

model2 -
x = merge([u, m], mode='concat')
x = Flatten()(x)
x = Dense(100, activation='relu')(x)
x = Dropout(0.5)(x)
x = Dense(5,activation='relu')(x)
x = Dense(1)(x)
nn = Model([user_in, movie_in], x)
nn.compile(Adam(0.001), loss='mse')


val_loss on different - 
DATASETS
models    	 		   ml-small 		   	 ml-100k  			     ml-latest
original               0.8324                0.9019                  0.9068

model1                 0.8412                0.8835                  0.9142

model2                 0.8414                 0.8688                  0.8279

on increasing batch size we found that the execution became much faster


ORIGINAL ON ml-small

batch size - 64 - 12 epoch
Train on 80059 samples, validate on 19945 samples
Epoch 1/12
80059/80059 [==============================] - 10s - loss: 2.4770 - val_loss: 0.8999
Epoch 2/12
80059/80059 [==============================] - 10s - loss: 1.4785 - val_loss: 0.9163
Epoch 3/12
80059/80059 [==============================] - 10s - loss: 1.2267 - val_loss: 0.8705
Epoch 4/12
80059/80059 [==============================] - 10s - loss: 1.0268 - val_loss: 0.8508
Epoch 5/12
80059/80059 [==============================] - 10s - loss: 0.9032 - val_loss: 0.8475
Epoch 6/12
80059/80059 [==============================] - 10s - loss: 0.8400 - val_loss: 0.8327
Epoch 7/12
80059/80059 [==============================] - 10s - loss: 0.8157 - val_loss: 0.8324
Epoch 8/12
80059/80059 [==============================] - 10s - loss: 0.8054 - val_loss: 0.8348
Epoch 9/12
80059/80059 [==============================] - 10s - loss: 0.8026 - val_loss: 0.8457
Epoch 10/12
80059/80059 [==============================] - 10s - loss: 0.7996 - val_loss: 0.8374
Epoch 11/12
80059/80059 [==============================] - 10s - loss: 0.7991 - val_loss: 0.8370
Epoch 12/12
80059/80059 [==============================] - 10s - loss: 0.7958 - val_loss: 0.8371


batch size - 64 epoch - 8
Train on 80059 samples, validate on 19945 samples
Epoch 1/8
80059/80059 [==============================] - 10s - loss: 2.4973 - val_loss: 0.9195
Epoch 2/8
80059/80059 [==============================] - 10s - loss: 1.4545 - val_loss: 0.8741
Epoch 3/8
80059/80059 [==============================] - 10s - loss: 1.2193 - val_loss: 0.8621
Epoch 4/8
80059/80059 [==============================] - 10s - loss: 1.0306 - val_loss: 0.8525
Epoch 5/8
80059/80059 [==============================] - 10s - loss: 0.9037 - val_loss: 0.8371
Epoch 6/8
80059/80059 [==============================] - 10s - loss: 0.8415 - val_loss: 0.8366
Epoch 7/8
80059/80059 [==============================] - 10s - loss: 0.8169 - val_loss: 0.8363
Epoch 8/8
80059/80059 [==============================] - 10s - loss: 0.8051 - val_loss: 0.8409

batch size-512-60 epoch
Train on 80059 samples, validate on 19945 samples
Epoch 1/60
80059/80059 [==============================] - 1s - loss: 4.5729 - val_loss: 1.0060
Epoch 2/60
80059/80059 [==============================] - 1s - loss: 1.9441 - val_loss: 0.9647
Epoch 3/60
80059/80059 [==============================] - 1s - loss: 1.8673 - val_loss: 0.9540
Epoch 4/60
80059/80059 [==============================] - 1s - loss: 1.7707 - val_loss: 0.9440
Epoch 5/60
80059/80059 [==============================] - 1s - loss: 1.7223 - val_loss: 0.9636
Epoch 6/60
80059/80059 [==============================] - 1s - loss: 1.6910 - val_loss: 0.9541
Epoch 7/60
80059/80059 [==============================] - 1s - loss: 1.6414 - val_loss: 0.9500
Epoch 8/60
80059/80059 [==============================] - 1s - loss: 1.6079 - val_loss: 0.9369
Epoch 9/60
80059/80059 [==============================] - ETA: 0s - loss: 1.542 - 1s - loss: 1.5440 - val_loss: 0.9213
Epoch 10/60
80059/80059 [==============================] - 1s - loss: 1.5018 - val_loss: 0.9167
Epoch 11/60
80059/80059 [==============================] - 1s - loss: 1.4628 - val_loss: 0.9022
Epoch 12/60
80059/80059 [==============================] - 1s - loss: 1.4195 - val_loss: 0.9213
Epoch 13/60
80059/80059 [==============================] - 1s - loss: 1.3729 - val_loss: 0.9115
Epoch 14/60
80059/80059 [==============================] - 1s - loss: 1.3253 - val_loss: 0.9062
Epoch 15/60
80059/80059 [==============================] - 1s - loss: 1.2893 - val_loss: 0.8828
Epoch 16/60
80059/80059 [==============================] - 1s - loss: 1.2538 - val_loss: 0.8929
Epoch 17/60
80059/80059 [==============================] - 1s - loss: 1.2093 - val_loss: 0.8736
Epoch 18/60
80059/80059 [==============================] - 1s - loss: 1.1698 - val_loss: 0.8994
Epoch 19/60
80059/80059 [==============================] - 1s - loss: 1.1381 - val_loss: 0.8850
Epoch 20/60
80059/80059 [==============================] - 1s - loss: 1.1035 - val_loss: 0.8774
Epoch 21/60
80059/80059 [==============================] - 1s - loss: 1.0699 - val_loss: 0.8569
Epoch 22/60
80059/80059 [==============================] - 1s - loss: 1.0350 - val_loss: 0.8553
Epoch 23/60
80059/80059 [==============================] - 1s - loss: 1.0002 - val_loss: 0.8649
Epoch 24/60
80059/80059 [==============================] - 1s - loss: 0.9639 - val_loss: 0.8549
Epoch 25/60
80059/80059 [==============================] - 1s - loss: 0.9423 - val_loss: 0.8529
Epoch 26/60
80059/80059 [==============================] - ETA: 0s - loss: 0.923 - 1s - loss: 0.9239 - val_loss: 0.8439
Epoch 27/60
80059/80059 [==============================] - 1s - loss: 0.8993 - val_loss: 0.8426
Epoch 28/60
80059/80059 [==============================] - 1s - loss: 0.8739 - val_loss: 0.8423
Epoch 29/60
80059/80059 [==============================] - 1s - loss: 0.8560 - val_loss: 0.8377
Epoch 30/60
80059/80059 [==============================] - 2s - loss: 0.8380 - val_loss: 0.8408
Epoch 31/60
80059/80059 [==============================] - 1s - loss: 0.8222 - val_loss: 0.8370
Epoch 32/60
80059/80059 [==============================] - 1s - loss: 0.8039 - val_loss: 0.8331
Epoch 33/60
80059/80059 [==============================] - 1s - loss: 0.7899 - val_loss: 0.8330
Epoch 34/60
80059/80059 [==============================] - 1s - loss: 0.7879 - val_loss: 0.8297
Epoch 35/60
80059/80059 [==============================] - 1s - loss: 0.7716 - val_loss: 0.8338
Epoch 36/60
80059/80059 [==============================] - 1s - loss: 0.7641 - val_loss: 0.8309
Epoch 37/60
80059/80059 [==============================] - 1s - loss: 0.7596 - val_loss: 0.8290
Epoch 38/60
80059/80059 [==============================] - 1s - loss: 0.7507 - val_loss: 0.8293ss: 0.75
Epoch 39/60
80059/80059 [==============================] - 1s - loss: 0.7445 - val_loss: 0.8282
Epoch 40/60
80059/80059 [==============================] - 1s - loss: 0.7431 - val_loss: 0.8252s
Epoch 41/60
80059/80059 [==============================] - 1s - loss: 0.7376 - val_loss: 0.8251
Epoch 42/60
80059/80059 [==============================] - 1s - loss: 0.7373 - val_loss: 0.8284
Epoch 43/60
80059/80059 [==============================] - 1s - loss: 0.7328 - val_loss: 0.8287
Epoch 44/60
80059/80059 [==============================] - 1s - loss: 0.7284 - val_loss: 0.8284
Epoch 45/60
80059/80059 [==============================] - 1s - loss: 0.7288 - val_loss: 0.8288
Epoch 46/60
80059/80059 [==============================] - 1s - loss: 0.7253 - val_loss: 0.8280
Epoch 47/60
80059/80059 [==============================] - 1s - loss: 0.7244 - val_loss: 0.8270
Epoch 48/60
80059/80059 [==============================] - 1s - loss: 0.7263 - val_loss: 0.8302
Epoch 49/60
80059/80059 [==============================] - 1s - loss: 0.7224 - val_loss: 0.8303
Epoch 50/60
80059/80059 [==============================] - 1s - loss: 0.7192 - val_loss: 0.8317
Epoch 51/60
80059/80059 [==============================] - 1s - loss: 0.7212 - val_loss: 0.8270
Epoch 52/60
80059/80059 [==============================] - 1s - loss: 0.7179 - val_loss: 0.8327
Epoch 53/60
80059/80059 [==============================] - 1s - loss: 0.7191 - val_loss: 0.8307
Epoch 54/60
80059/80059 [==============================] - 1s - loss: 0.7177 - val_loss: 0.8311
Epoch 55/60
80059/80059 [==============================] - 1s - loss: 0.7168 - val_loss: 0.8299
Epoch 56/60
80059/80059 [==============================] - 1s - loss: 0.7153 - val_loss: 0.8336
Epoch 57/60
80059/80059 [==============================] - 1s - loss: 0.7152 - val_loss: 0.8321
Epoch 58/60
80059/80059 [==============================] - 1s - loss: 0.7156 - val_loss: 0.8331
Epoch 59/60
80059/80059 [==============================] - 1s - loss: 0.7135 - val_loss: 0.8354
Epoch 60/60
80059/80059 [==============================] - 1s - loss: 0.7130 - val_loss: 0.8352



batch size - 128 epoch - 8
Train on 80059 samples, validate on 19945 samples
Epoch 1/8
80059/80059 [==============================] - 5s - loss: 3.0361 - val_loss: 0.9479
Epoch 2/8
80059/80059 [==============================] - 5s - loss: 1.6123 - val_loss: 0.8993
Epoch 3/8
80059/80059 [==============================] - 5s - loss: 1.4646 - val_loss: 0.9067
Epoch 4/8
80059/80059 [==============================] - 5s - loss: 1.3330 - val_loss: 0.8733
Epoch 5/8
80059/80059 [==============================] - 5s - loss: 1.1966 - val_loss: 0.8657
Epoch 6/8
80059/80059 [==============================] - 5s - loss: 1.0997 - val_loss: 0.8512
Epoch 7/8
80059/80059 [==============================] - 5s - loss: 1.0092 - val_loss: 0.8487
Epoch 8/8
80059/80059 [==============================] - 5s - loss: 0.9375 - val_loss: 0.8510

batch size - 32 8 epoch
Train on 80059 samples, validate on 19945 samples
Epoch 1/8
80059/80059 [==============================] - 20s - loss: 2.0545 - val_loss: 0.8869
Epoch 2/8
80059/80059 [==============================] - 20s - loss: 1.1910 - val_loss: 0.8616
Epoch 3/8
80059/80059 [==============================] - 19s - loss: 0.9428 - val_loss: 0.8522
Epoch 4/8
80059/80059 [==============================] - 19s - loss: 0.8563 - val_loss: 0.8431
Epoch 5/8
80059/80059 [==============================] - 20s - loss: 0.8366 - val_loss: 0.8424
Epoch 6/8
80059/80059 [==============================] - 19s - loss: 0.8315 - val_loss: 0.8394
Epoch 7/8
80059/80059 [==============================] - 19s - loss: 0.8287 - val_loss: 0.8358
Epoch 8/8
80059/80059 [==============================] - 20s - loss: 0.8255 - val_loss: 0.8368

256-8
Train on 80059 samples, validate on 19945 samples
Epoch 1/8
80059/80059 [==============================] - 2s - loss: 3.7104 - val_loss: 0.9885
Epoch 2/8
80059/80059 [==============================] - 3s - loss: 1.8375 - val_loss: 0.9544
Epoch 3/8
80059/80059 [==============================] - 2s - loss: 1.7382 - val_loss: 0.9510
Epoch 4/8
80059/80059 [==============================] - 2s - loss: 1.6570 - val_loss: 0.9083
Epoch 5/8
80059/80059 [==============================] - 3s - loss: 1.5724 - val_loss: 0.9242
Epoch 6/8
80059/80059 [==============================] - 2s - loss: 1.4807 - val_loss: 0.9434
Epoch 7/8
80059/80059 [==============================] - 2s - loss: 1.3946 - val_loss: 0.9171
Epoch 8/8
80059/80059 [==============================] - 2s - loss: 1.3261 - val_loss: 0.8906


128 batch size 20 epoch
Train on 80059 samples, validate on 19945 samples
Epoch 1/20
80059/80059 [==============================] - 5s - loss: 2.9679 - val_loss: 0.9449
Epoch 2/20
80059/80059 [==============================] - 5s - loss: 1.6079 - val_loss: 0.8890
Epoch 3/20
80059/80059 [==============================] - 5s - loss: 1.4628 - val_loss: 0.8912
Epoch 4/20
80059/80059 [==============================] - 5s - loss: 1.3337 - val_loss: 0.8646
Epoch 5/20
80059/80059 [==============================] - 5s - loss: 1.1990 - val_loss: 0.8675
Epoch 6/20
80059/80059 [==============================] - 5s - loss: 1.1001 - val_loss: 0.8633
Epoch 7/20
80059/80059 [==============================] - 5s - loss: 1.0052 - val_loss: 0.8512
Epoch 8/20
80059/80059 [==============================] - 5s - loss: 0.9251 - val_loss: 0.8377
Epoch 9/20
80059/80059 [==============================] - 5s - loss: 0.8695 - val_loss: 0.8346
Epoch 10/20
80059/80059 [==============================] - 5s - loss: 0.8300 - val_loss: 0.8378
Epoch 11/20
80059/80059 [==============================] - 5s - loss: 0.8051 - val_loss: 0.8340
Epoch 12/20
80059/80059 [==============================] - 5s - loss: 0.7880 - val_loss: 0.8322
Epoch 13/20
80059/80059 [==============================] - 5s - loss: 0.7801 - val_loss: 0.8342
Epoch 14/20
80059/80059 [==============================] - 5s - loss: 0.7786 - val_loss: 0.8327
Epoch 15/20
80059/80059 [==============================] - 5s - loss: 0.7762 - val_loss: 0.8331
Epoch 16/20
80059/80059 [==============================] - 5s - loss: 0.7737 - val_loss: 0.8333
Epoch 17/20
80059/80059 [==============================] - 5s - loss: 0.7713 - val_loss: 0.8347
Epoch 18/20
80059/80059 [==============================] - 5s - loss: 0.7698 - val_loss: 0.8348
Epoch 19/20
80059/80059 [==============================] - 5s - loss: 0.7690 - val_loss: 0.8350
Epoch 20/20
80059/80059 [==============================] - 5s - loss: 0.7662 - val_loss: 0.8326


batch size -1024 - 80 epoch
Train on 80059 samples, validate on 19945 samples
Epoch 1/80
80059/80059 [==============================] - 1s - loss: 7.8625 - val_loss: 1.1569
Epoch 2/80
80059/80059 [==============================] - 1s - loss: 2.1071 - val_loss: 0.9956
Epoch 3/80
80059/80059 [==============================] - 1s - loss: 1.9312 - val_loss: 0.9719
Epoch 4/80
80059/80059 [==============================] - 1s - loss: 1.8677 - val_loss: 0.9466
Epoch 5/80
80059/80059 [==============================] - 1s - loss: 1.8414 - val_loss: 0.9875
Epoch 6/80
80059/80059 [==============================] - 1s - loss: 1.8212 - val_loss: 0.9734
Epoch 7/80
80059/80059 [==============================] - 1s - loss: 1.8105 - val_loss: 0.9506
Epoch 8/80
80059/80059 [==============================] - 1s - loss: 1.7583 - val_loss: 0.9626
Epoch 9/80
80059/80059 [==============================] - 1s - loss: 1.7490 - val_loss: 0.9823
Epoch 10/80
80059/80059 [==============================] - 1s - loss: 1.7321 - val_loss: 0.9788
Epoch 11/80
80059/80059 [==============================] - 1s - loss: 1.7208 - val_loss: 0.9536
Epoch 12/80
80059/80059 [==============================] - 1s - loss: 1.6931 - val_loss: 0.9618
Epoch 13/80
80059/80059 [==============================] - 1s - loss: 1.6782 - val_loss: 0.9627
Epoch 14/80
80059/80059 [==============================] - 1s - loss: 1.6478 - val_loss: 0.9453
Epoch 15/80
80059/80059 [==============================] - 1s - loss: 1.6341 - val_loss: 0.9432
Epoch 16/80
80059/80059 [==============================] - 1s - loss: 1.6001 - val_loss: 0.9458
Epoch 17/80
80059/80059 [==============================] - 1s - loss: 1.5856 - val_loss: 0.9474
Epoch 18/80
80059/80059 [==============================] - 1s - loss: 1.5652 - val_loss: 0.9342
Epoch 19/80
80059/80059 [==============================] - 1s - loss: 1.5299 - val_loss: 0.9236
Epoch 20/80
80059/80059 [==============================] - 1s - loss: 1.5022 - val_loss: 0.9307
Epoch 21/80
80059/80059 [==============================] - 1s - loss: 1.4828 - val_loss: 0.9165ss: - ETA: 0s - los
Epoch 22/80
80059/80059 [==============================] - ETA: 0s - loss: 1.457 - 1s - loss: 1.4571 - val_loss: 0.9068
Epoch 23/80
80059/80059 [==============================] - 1s - loss: 1.4208 - val_loss: 0.9137
Epoch 24/80
80059/80059 [==============================] - 1s - loss: 1.3972 - val_loss: 0.9158
Epoch 25/80
80059/80059 [==============================] - 1s - loss: 1.3817 - val_loss: 0.9156
Epoch 26/80
80059/80059 [==============================] - 1s - loss: 1.3592 - val_loss: 0.9036
Epoch 27/80
80059/80059 [==============================] - 1s - loss: 1.3292 - val_loss: 0.8964
Epoch 28/80
80059/80059 [==============================] - 1s - loss: 1.3201 - val_loss: 0.8895
Epoch 29/80
80059/80059 [==============================] - 1s - loss: 1.2811 - val_loss: 0.8828ss: 1.28
Epoch 30/80
80059/80059 [==============================] - 1s - loss: 1.2676 - val_loss: 0.8865
Epoch 31/80
80059/80059 [==============================] - 1s - loss: 1.2521 - val_loss: 0.8786
Epoch 32/80
80059/80059 [==============================] - 1s - loss: 1.2364 - val_loss: 0.8865
Epoch 33/80
80059/80059 [==============================] - 1s - loss: 1.2108 - val_loss: 0.8815
Epoch 34/80
80059/80059 [==============================] - 1s - loss: 1.1875 - val_loss: 0.8770ss: 1.188
Epoch 35/80
80059/80059 [==============================] - ETA: 0s - loss: 1.1632- ETA: - 1s - loss: 1.1634 - val_loss: 0.8739
Epoch 36/80
80059/80059 [==============================] - 1s - loss: 1.1425 - val_loss: 0.8747
Epoch 37/80
80059/80059 [==============================] - 1s - loss: 1.1248 - val_loss: 0.8689
Epoch 38/80
80059/80059 [==============================] - 1s - loss: 1.1098 - val_loss: 0.8706
Epoch 39/80
80059/80059 [==============================] - 1s - loss: 1.0912 - val_loss: 0.8653
Epoch 40/80
80059/80059 [==============================] - 1s - loss: 1.0698 - val_loss: 0.8596
Epoch 41/80
80059/80059 [==============================] - 1s - loss: 1.0512 - val_loss: 0.8645
Epoch 42/80
80059/80059 [==============================] - 1s - loss: 1.0295 - val_loss: 0.8661
Epoch 43/80
80059/80059 [==============================] - 1s - loss: 1.0154 - val_loss: 0.8506
Epoch 44/80
80059/80059 [==============================] - 1s - loss: 0.9971 - val_loss: 0.8547
Epoch 45/80
80059/80059 [==============================] - ETA: 0s - loss: 0.9850- ETA: 0s - l - 1s - loss: 0.9851 - val_loss: 0.8565
Epoch 46/80
80059/80059 [==============================] - 1s - loss: 0.9674 - val_loss: 0.8472
Epoch 47/80
80059/80059 [==============================] - 1s - loss: 0.9490 - val_loss: 0.8468
Epoch 48/80
80059/80059 [==============================] - 1s - loss: 0.9411 - val_loss: 0.8420ss: 0.92 - ETA: 0s - loss
Epoch 49/80
80059/80059 [==============================] - 1s - loss: 0.9270 - val_loss: 0.8531
Epoch 50/80
80059/80059 [==============================] - 1s - loss: 0.9052 - val_loss: 0.8482
Epoch 51/80
80059/80059 [==============================] - 1s - loss: 0.8957 - val_loss: 0.8457
Epoch 52/80
80059/80059 [==============================] - 1s - loss: 0.8860 - val_loss: 0.8470
Epoch 53/80
80059/80059 [==============================] - 1s - loss: 0.8699 - val_loss: 0.8435
Epoch 54/80
80059/80059 [==============================] - 1s - loss: 0.8587 - val_loss: 0.8420
Epoch 55/80
80059/80059 [==============================] - 1s - loss: 0.8499 - val_loss: 0.8414
Epoch 56/80
80059/80059 [==============================] - 1s - loss: 0.8425 - val_loss: 0.8452
Epoch 57/80
80059/80059 [==============================] - 1s - loss: 0.8324 - val_loss: 0.8425
Epoch 58/80
80059/80059 [==============================] - 1s - loss: 0.8235 - val_loss: 0.8359ss: - ETA: 0s - l
Epoch 59/80
80059/80059 [==============================] - 1s - loss: 0.8135 - val_loss: 0.8408
Epoch 60/80
80059/80059 [==============================] - 1s - loss: 0.8062 - val_loss: 0.8351
Epoch 61/80
80059/80059 [==============================] - 1s - loss: 0.8012 - val_loss: 0.8403
Epoch 62/80
80059/80059 [==============================] - 1s - loss: 0.7899 - val_loss: 0.8372ss:  - ETA: 0s - loss: 0.7 - ETA: 0s - loss: 0
Epoch 63/80
80059/80059 [==============================] - 1s - loss: 0.7828 - val_loss: 0.8401
Epoch 64/80
80059/80059 [==============================] - 1s - loss: 0.7790 - val_loss: 0.8376
Epoch 65/80
80059/80059 [==============================] - 1s - loss: 0.7726 - val_loss: 0.8357
Epoch 66/80
80059/80059 [==============================] - 1s - loss: 0.7668 - val_loss: 0.8333ss: 
Epoch 67/80
80059/80059 [==============================] - 1s - loss: 0.7600 - val_loss: 0.8333ss: 0.757 - ETA: 0s - loss: 0.7
Epoch 68/80
80059/80059 [==============================] - 1s - loss: 0.7561 - val_loss: 0.8319
Epoch 69/80
80059/80059 [==============================] - ETA: 0s - loss: 0.7519- ETA: 0s - - 1s - loss: 0.7520 - val_loss: 0.8329
Epoch 70/80
80059/80059 [==============================] - 1s - loss: 0.7442 - val_loss: 0.8326
Epoch 71/80
80059/80059 [==============================] - 1s - loss: 0.7402 - val_loss: 0.8333
Epoch 72/80
80059/80059 [==============================] - 1s - loss: 0.7394 - val_loss: 0.8288ss: 0.72 - ETA: 0s -
Epoch 73/80
80059/80059 [==============================] - 1s - loss: 0.7353 - val_loss: 0.8306ss:
Epoch 74/80
80059/80059 [==============================] - 1s - loss: 0.7295 - val_loss: 0.8325
Epoch 75/80
80059/80059 [==============================] - 1s - loss: 0.7270 - val_loss: 0.8321
Epoch 76/80
80059/80059 [==============================] - 1s - loss: 0.7264 - val_loss: 0.8302
Epoch 77/80
80059/80059 [==============================] - 1s - loss: 0.7214 - val_loss: 0.8320ss: 
Epoch 78/80
80059/80059 [==============================] - 1s - loss: 0.7177 - val_loss: 0.8314
Epoch 79/80
80059/80059 [==============================] - 1s - loss: 0.7154 - val_loss: 0.8299ss
Epoch 80/80
80059/80059 [==============================] - 1s - loss: 0.7159 - val_loss: 0.8326

